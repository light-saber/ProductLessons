# AI Tokens & Context Windows: The Hidden Constraint Every AI PM Must Understand

**Topic:** AI Tokens & Context Windows  
**Date:** 2026-02-19  
**Level:** Principal PM

---

## üî¨ Technical Explanation

**What is a token?**

A token is the atomic unit of text that LLMs process. It's not a word, nor a character‚Äîit's somewhere in between. Roughly:
- 1 token ‚âà 0.75 words (English)
- 1 token ‚âà 4 characters
- "ChatGPT" = 2 tokens ("Chat" + "GPT")
- "hello" = 1 token
- "The quick brown fox" ‚âà 5 tokens

**Context Window = The Working Memory of AI**

The context window is the maximum number of tokens an LLM can process in a single request. This includes:
- Your prompt (input tokens)
- The model's response (output tokens)
- System instructions
- Conversation history

**Current Limits (as of Feb 2026):**
| Model | Context Window | Cost per 1M tokens (input/output) |
|-------|----------------|-----------------------------------|
| GPT-4o | 128K | $2.50 / $10.00 |
| Claude 3.5 Sonnet | 200K | $3.00 / $15.00 |
| Gemini 1.5 Pro | 1M | $3.50 / $10.50 |
| Llama 3.1 405B | 128K | ~$0.50 / $1.00 (self-hosted) |
| GPT-4o-mini | 128K | $0.15 / $0.60 |

**The Math That Matters:**
- A 100-page PDF ‚âà 25K-50K tokens
- 1 hour of meeting transcript ‚âà 10K-15K tokens
- Code review of a large PR ‚âà 5K-20K tokens

When you exceed the context window, you must either:
1. Truncate (lose information)
2. Chunk (process in pieces)
3. Use a model with a larger window (pay more)

---

## üçé Simple Explanation

**Think of tokens as "chunks of meaning" and context window as "short-term memory."**

Imagine you're having a conversation with a colleague who has perfect recall‚Äîbut only for the last 10 minutes. As soon as you exceed their mental capacity, they start forgetting the beginning of your discussion.

**The Grocery Store Analogy:**
- **Tokens = Items on your shopping list**
- **Context window = Size of your shopping basket**

You can buy anything you want, but you can only carry what fits in your basket. If you try to carry more, things fall out (truncation) or you make multiple trips (chunking), which takes more time and energy (cost & latency).

**Why PMs should care:**
- Every feature that uses long documents, transcripts, or history hits this wall
- The cost isn't linear‚Äîlonger context often means exponentially slower responses
- Users don't understand why their 200-page PDF won't summarize in one go

---

## üíº Real Example: The Document Q&A Feature

**Scenario:** You're a PM at Walmart building an internal tool that lets store managers upload any PDF (handbooks, policy docs, supplier contracts) and ask questions about it.

**The Product Requirement:**
> "Users should be able to upload PDFs up to 500 pages and ask any question."

**The Engineering Reality:**
- 500 pages ‚âà 125K-250K tokens
- GPT-4o's 128K window is barely enough, and expensive
- Response time at full context: 30-60 seconds (unacceptable)

**The PM Decision Using Token Knowledge:**

Instead of blindly accepting the requirement, you work with engineering on a **RAG-based architecture:**

1. **Chunk the document** into 1K-token segments
2. **Embed chunks** into a vector database
3. **Retrieve only relevant chunks** (2-4K tokens max) based on the question
4. **Send only relevant context** to the LLM

**Result:**
- Cost drops by 80% (from ~$0.50 to ~$0.08 per query)
- Response time: 3-5 seconds (acceptable)
- Supports unlimited document size
- Maintains 95% answer accuracy vs. full-context approach

**The PM who didn't know about tokens** would have:
- Promised full-document processing
- Built on GPT-4o at full context
- Shipped a product that's too slow and too expensive
- Faced angry users and budget overruns

---

## üéØ Challenge: The Token Math Quiz

**Scenario:** You're building a customer support chatbot. You want to:
- Include the last 10 messages of conversation history
- Add your 500-word knowledge base article
- Keep response under 200 tokens

**Question 1:** Roughly how many input tokens is this? (Use 1 token ‚âà 0.75 words)

<details>
<summary>Click for answer</summary>

- 10 messages √ó 30 words avg = 300 words ‚âà **400 tokens**
- 500-word article ‚âà **670 tokens**
- System prompt ‚âà **100 tokens**
- **Total input: ~1,170 tokens**
- Plus 200 output tokens = **~1,370 tokens total**

This easily fits in any modern LLM's context window. ‚úÖ
</details>

**Question 2:** If you use GPT-4o-mini at $0.15/1M input tokens and $0.60/1M output tokens, what's the cost per query?

<details>
<summary>Click for answer</summary>

- Input: 1,170 tokens √ó $0.15 / 1,000,000 = **$0.000175**
- Output: 200 tokens √ó $0.60 / 1,000,000 = **$0.00012**
- **Total: ~$0.0003 per query (0.03 cents)**

At 10,000 queries/day: **$3/day or ~$90/month**

Compare to GPT-4o: ~$30/day or ~$900/month
</details>

**Question 3 (Spot the AI Slop):** 

A vendor pitches you: *"Our AI reads your entire 1,000-page legal contract in one pass and summarizes it instantly!"*

What's wrong with this claim?

<details>
<summary>Click for answer</summary>

**Red flags:**
1. **1,000 pages ‚âà 250K-500K tokens** ‚Äî exceeds most LLM context windows
2. **"Instantly"** ‚Äî even with 1M-token models, processing takes 20-60 seconds
3. **Cost** ‚Äî at ~$10 per 1M tokens output, this single query could cost $5-15
4. **Accuracy** ‚Äî LLMs "lose focus" in the middle of very long contexts (the "lost in the middle" problem)

**Better approach:** Chunk + RAG, or use specialized document AI (like Google's Document AI)
</details>

---

## üìù Key Takeaways

1. **Tokens are the currency of AI** ‚Äî understand them to estimate cost and feasibility
2. **Context window is a hard constraint** ‚Äî design around it, not into it
3. **Longer context = slower & pricier** ‚Äî always consider chunking/RAG for large documents
4. **Different models, different limits** ‚Äî match the model to your use case
5. **Users don't know about tokens** ‚Äî abstract this complexity away in your UX

---

**Source:** OpenAI Tokenizer documentation, Anthropic context window docs, "Lost in the Middle" research paper (Stanford/UC Berkeley), personal experience building RAG systems at scale.

**Next Lesson:** *LLM Models Compared: GPT vs Claude vs Gemini vs Llama ‚Äî When to Use What*
