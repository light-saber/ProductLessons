# Model Parameters & Quantization: Running Big AI on Small Budgets

**Topic:** Model Parameters & Quantization (4-bit, 8-bit, GGUF)  
**Date:** 2026-02-23  
**Level:** Principal PM

---

## 1. Technical Explanation

A model's **parameters** are its learned weights — Llama 3 70B has 70 billion floating-point numbers. By default, each is stored in 16-bit (FP16), so 70B params ≈ **140 GB of GPU memory** just to load.

**Quantization** shrinks those numbers to fewer bits:

- **FP16 → INT8 (8-bit):** Halves memory (~70 GB). Minimal quality loss (<1% on benchmarks).
- **INT4 (4-bit):** Quarters it (~35 GB). Fits 70B on a single 48 GB GPU. Quality drops ~2-5% on reasoning tasks.
- **GGUF** is a file format (by llama.cpp) that packages quantized weights for efficient CPU/GPU inference — it's why you can run Llama on a MacBook.

Key methods: **GPTQ** (GPU-optimized, needs calibration data), **AWQ** (activation-aware, better quality), **bitsandbytes** (on-the-fly, easiest). The tradeoff is always **memory/cost vs. quality**. Smaller quants are faster and cheaper but degrade on complex reasoning and long outputs.

---

## 2. Simple Explanation

Imagine a high-resolution photo. It looks great but takes 140 MB. You compress it to a JPEG — now it's 35 MB and *almost* as sharp. That's quantization for AI models.

The "parameters" are the model's knowledge — billions of numbers it learned during training. Normally each number is very precise (16-bit), but you can round them to be less precise (8-bit or 4-bit). The model gets much smaller and faster, with only a slight drop in smarts.

This is why apps like **Jan**, **Ollama**, and **LM Studio** can run powerful AI models on your laptop — they use quantized versions. For a product team, this means you might not always need expensive cloud GPUs.

---

## 3. Real Example

**Walmart's search & catalog team** could deploy a quantized Llama 70B (4-bit, ~35 GB) on a single A6000 GPU for internal product-attribute extraction instead of calling GPT-4 via API. At 50M daily catalog queries, the API cost would be ~$15K/day. A self-hosted quantized model costs ~$2K/month in GPU rental. The PM's decision: accept a ~3% accuracy drop on edge-case attributes to save **$400K+/month**. They validate with an eval set comparing INT4 vs FP16 outputs — if accuracy stays above 95%, ship the quantized version.

---

## 4. Key Takeaways

- **Parameters = model size.** 7B fits a laptop; 70B needs serious GPUs; 400B+ needs clusters.
- **Quantization trades precision for efficiency.** 8-bit is nearly lossless; 4-bit is the sweet spot for cost savings.
- **GGUF is the format that unlocked local AI.** If your product needs on-device inference, you'll use quantized models.
- **Always eval before shipping a quantized model.** Run your actual use-case through both versions — aggregate benchmarks lie.
- **This is a PM decision, not just engineering.** You're choosing the cost-quality tradeoff for your product.
