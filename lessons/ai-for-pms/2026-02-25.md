# RAG & Vector Databases: Teaching Your AI to Use Company Knowledge

**Topic:** RAG & Vector Databases  
**Date:** February 25, 2026  
**Lesson 5 of 12**

---

## 1. Technical Explanation

Retrieval-Augmented Generation (RAG) is an architecture where an LLM's response is grounded in externally retrieved documents rather than relying solely on its training data. The pipeline works in three steps: **embed → retrieve → generate**.

First, your documents are chunked (typically 256–512 tokens) and converted into high-dimensional vectors (embeddings) using models like OpenAI's `text-embedding-3-small` (1536 dimensions) or Cohere's Embed v3. These vectors are stored in a **vector database** — purpose-built datastores like Pinecone, Weaviate, Qdrant, or pgvector (Postgres extension) that perform fast approximate nearest-neighbor (ANN) search.

At query time, the user's question is embedded, the top-k most similar chunks are retrieved (typically k=5–20), and these are injected into the LLM's prompt as context. The model then generates an answer grounded in those documents.

Key metrics: embedding latency (~10–50ms), retrieval recall@10 (aim for >90%), and chunk relevance precision. Poor chunking strategy is the #1 cause of bad RAG systems — not the LLM.

## 2. Simple Explanation

Imagine you're a brilliant consultant who's never worked at Walmart. You're smart, but you don't know Walmart's internal policies, last quarter's numbers, or the org chart. Now imagine before every meeting, an assistant hands you exactly the right 5 pages from Walmart's internal wiki. Suddenly, you sound like a company insider.

That's RAG. The LLM is the consultant. The vector database is the filing system. The retrieval step is the assistant who finds the right pages. Without RAG, the AI makes things up. With RAG, it answers from *your* data — return policies, product catalogs, internal docs — accurately and verifiably.

## 3. Real Example

Notion AI uses RAG to answer questions about your workspace. When you ask "What did the team decide about the Q3 roadmap?", it doesn't hallucinate — it retrieves relevant Notion pages, feeds them to the LLM, and cites sources.

The PM decision here was critical: **what gets indexed and how?** Notion chose to respect permissions (you only retrieve docs you have access to), chunk by content blocks rather than fixed token counts, and re-index on every edit. They also decided *not* to fine-tune a model on user data — RAG was cheaper, faster to update, and didn't require training infrastructure. At Walmart, the same pattern applies: imagine a store associate asking an AI about return policies. RAG over the policy database beats fine-tuning every time — policies change weekly, and RAG reflects changes instantly.

## 4. Key Takeaways

- **RAG = retrieval + generation.** The retrieval quality matters more than the model choice — garbage in, garbage out.
- **Vector databases** enable semantic search (meaning-based, not keyword-based). They're the backbone of any knowledge-grounded AI product.
- **Chunking strategy is your biggest lever.** Too small = no context; too large = noise drowns signal. Test empirically.
- **RAG beats fine-tuning for dynamic data.** If your content changes frequently (policies, catalogs, docs), RAG updates instantly; fine-tuning requires retraining.
- **Always surface citations.** RAG lets you point users to source documents — this builds trust and reduces hallucination risk.
