# Latency, Throughput & Cost Optimization: The AI PM's Performance Triangle

**Topic:** Latency, Throughput & Cost Optimization  
**Date:** 2026-02-27  
**Series:** AI for PMs — Day 7

---

## 1. Technical Explanation

Every LLM API call has two phases: **prefill** (processing your prompt — compute-bound) and **decode** (generating tokens one-by-one — memory-bound). Latency is time-to-first-token (TTFT) plus per-token generation time. Throughput is tokens/second across concurrent users.

The key levers PMs should know:

- **Model routing/cascading:** Route simple queries to smaller models (e.g., GPT-4o Mini, Claude Haiku), complex ones to flagship models. Saves 30–60% cost with minimal quality loss.
- **KV-cache & PagedAttention:** Frameworks like vLLM cache intermediate computations, cutting GPU memory waste ~50% and boosting throughput 2–5x via continuous batching.
- **Speculative decoding:** A small "draft" model predicts tokens; the large model verifies in parallel. 2–3x speedup, zero quality loss.
- **Semantic caching:** Cache responses to similar queries. Huge savings for repetitive use cases (FAQs, product search).
- **Quantization** (covered in Day 3): INT8 halves memory, INT4 quarters it — enabling cheaper hardware.

The cost equation: **cost = (input tokens × input price) + (output tokens × output price)**. Shorter prompts and constrained outputs directly reduce spend.

---

## 2. Simple Explanation

Think of an AI service like a restaurant kitchen. **Latency** is how long a single customer waits for their dish. **Throughput** is how many dishes the kitchen serves per hour. **Cost** is your grocery bill.

You can't optimize all three equally — it's a triangle of tradeoffs. Want faster responses? Use a bigger GPU (more cost). Want cheaper? Use a smaller model (maybe slower or less accurate). Want more users served? Batch orders together (slightly more wait per person).

The smartest restaurants have a **two-tier system**: the line cook handles simple orders (salads, drinks), while the head chef tackles complex dishes. That's exactly what model routing does — simple questions go to cheap, fast models; hard questions go to the expensive one.

---

## 3. Real Example

Walmart's product search AI likely handles millions of queries daily. Most are simple ("show me AA batteries") while some are complex ("compare organic baby formulas under ₹2000 with high DHA content").

A smart PM would implement **model cascading**: route ~80% of straightforward queries to a small, fast model (sub-200ms response, ~₹0.01/query) and escalate the complex 20% to a capable model (~800ms, ~₹0.15/query). Combined with semantic caching for repeated queries ("track my order"), this could cut inference costs by 50–70% while keeping p95 latency under 500ms. The PM's job: define the routing logic thresholds and monitor quality metrics at each tier.

---

## 4. Key Takeaways

- **Latency, throughput, and cost are a tradeoff triangle** — you can't max all three. Know which matters most for your feature.
- **Model routing is the highest-ROI optimization** a PM can push for — route 80% of traffic to cheap models, 20% to powerful ones.
- **Prompt length directly impacts cost and latency.** Shorter, structured prompts save money at scale.
- **Caching identical/similar queries** can eliminate 20–40% of API calls in high-traffic products.
- **Measure TTFT and tokens/second**, not just "response time" — they tell different stories about user experience.
