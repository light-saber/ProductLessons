# Evals & Benchmarking: How to Measure Whether Your AI Actually Works

**Topic:** Evals & Benchmarking  
**Date:** February 24, 2026  
**Lesson #4 in the AI for PMs series**

---

## 1. Technical Explanation

"Evals" (evaluations) are structured tests that measure an LLM's quality on specific tasks. They're the AI equivalent of unit tests — except the outputs are probabilistic, so you need statistical rigor.

The industry uses standardized benchmarks: **MMLU** (broad knowledge across 57 subjects), **HumanEval** (code generation), **GPQA** (graduate-level Q&A that even domain experts struggle with), **IFEval** (instruction-following precision), and **MATH Lvl 5** (competition-level math). Hugging Face's Open LLM Leaderboard evaluates models across six benchmarks using EleutherAI's evaluation harness.

But public benchmarks have a fatal flaw: **contamination**. Models may have trained on the test data, inflating scores. This is why Anthropic, OpenAI, and serious AI teams build **custom evals** — private test sets tailored to their actual use case. A "vibe check" isn't an eval. You need: (1) a curated dataset of input-output pairs, (2) a scoring function (exact match, LLM-as-judge, or human rating), and (3) statistical significance across enough samples (typically 200+). Metrics like **pass@k**, **F1**, and **win rate** matter more than a single accuracy number.

## 2. Simple Explanation

Imagine you're hiring a new employee. Their university grades (public benchmarks) tell you something — but not whether they'll excel at *your* specific job. So you give them a work sample test: real tasks from the role, scored by your team.

AI evals work the same way. Public leaderboards are like university rankings — useful for a rough filter. But what really matters is testing the model on *your* product's actual tasks. Does it summarize Walmart product reviews accurately? Does it classify returns correctly? You build a private "exam" with known right answers and grade the model on it. If it scores 85% today and 82% after a prompt change, you caught a regression before users did.

## 3. Real Example

When GitHub built Copilot, they couldn't just trust HumanEval scores. They created internal evals measuring **completion acceptance rate** — how often developers actually kept the suggestion. They also tracked **time-to-task-completion** in controlled studies. This told them something no benchmark could: GPT-4 generated more "correct" code but was slower, so for inline completions they used a smaller, faster model and reserved GPT-4 for chat.

At Walmart, if you're building an AI that answers seller queries, your eval would be 500 real seller questions with expert-verified answers, scored by an LLM-as-judge for accuracy and helpfulness. That's how you avoid shipping "AI slop" — you measure before you ship.

## 4. Key Takeaways

- **Public benchmarks are a starting filter, not a shipping decision.** Always build custom evals for your product's specific tasks.
- **An eval needs three things:** a curated dataset, a scoring method, and enough samples for statistical significance (~200+).
- **Benchmark contamination is real.** Models may score high on public tests simply because they trained on the answers.
- **LLM-as-judge** (using one model to grade another) is increasingly standard — cheaper than human review, more scalable, but needs calibration.
- **Track eval scores over time.** Every prompt change, model swap, or pipeline update should be regression-tested against your eval suite.
