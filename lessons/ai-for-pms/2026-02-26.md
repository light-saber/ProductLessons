# Fine-tuning vs Prompt Engineering vs RAG: Picking the Right Tool

**Topic:** Fine-tuning vs Prompt Engineering vs RAG  
**Date:** February 26, 2026  
**Lesson #6 in the AI for PMs series**

---

## 1. Technical Explanation

When customizing LLM behavior, PMs have three levers — each with different cost, complexity, and payoff profiles.

**Prompt Engineering** is the cheapest and fastest. You craft instructions, examples, and constraints in the prompt itself. Zero training cost, instant iteration. Works for 80%+ of use cases. Anthropic and OpenAI both recommend exhausting this approach first. Limitations: you're constrained by context window size, and complex behaviors may require verbose prompts that increase per-request token costs.

**RAG (Retrieval-Augmented Generation)** injects relevant external knowledge into the prompt at query time. Your model stays general-purpose, but gets grounded in your data — product catalogs, policy docs, knowledge bases. Cost scales with your vector DB and retrieval infra, not with model training. Latency adds 100-300ms for the retrieval step.

**Fine-tuning** trains a model on your specific data to internalize patterns, tone, or domain knowledge. OpenAI charges ~$8/1M training tokens for GPT-4o mini. It's powerful for consistent style, structured output formats, or niche domain expertise — but requires curated datasets (typically 50-1000+ examples), takes hours to train, and is hard to iterate on. Changes to behavior mean retraining.

**The decision framework:** Prompt engineering first → add RAG if the model needs external knowledge → fine-tune only if the first two can't achieve your quality bar.

## 2. Simple Explanation

Think of it like training a new employee at Walmart:

**Prompt engineering** = giving them a detailed instruction sheet each morning. Quick, flexible, easy to update. But you're limited by how much they can read before starting work.

**RAG** = giving them access to the company wiki so they can look things up while working. They don't memorize everything, but they can find the right answer when needed.

**Fine-tuning** = sending them through a specialized 3-month training program. They internalize the knowledge deeply, but it's expensive, slow, and you can't easily "retrain" them when policies change.

Most teams should start with the instruction sheet, add the wiki, and only invest in the training program when the first two aren't enough.

## 3. Real Example

GitHub Copilot uses all three. The base model is fine-tuned on code (billions of lines from GitHub). At query time, it uses RAG to pull context from your current repo — open files, imports, function signatures. And the system prompt engineers the model to produce code completions rather than explanations.

A PM decision here: when Copilot added "workspace" context (understanding your full repo), that was a RAG investment, not fine-tuning. Why? Repos change constantly — fine-tuning would be stale instantly. RAG lets the model adapt to your codebase in real-time. That's the kind of architectural decision an AI-literate PM needs to inform.

## 4. Key Takeaways

- **Start with prompt engineering** — it's free, fast, and handles most use cases
- **Add RAG when the model needs knowledge it wasn't trained on** — your company data, recent events, user-specific context
- **Fine-tune only as a last resort** — when you need consistent style, structured outputs, or deep domain behavior that prompts can't achieve
- **These aren't mutually exclusive** — production systems often use all three together
- **The PM's job:** match the technique to the problem's constraints (budget, latency, data freshness, quality bar)
