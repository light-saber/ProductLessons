# LLM Models: Capabilities, Costs & Tradeoffs Every AI PM Must Know

**Topic:** LLM Models (GPT, Claude, Gemini, Llama)  
**Date:** 2026-02-20  
**Level:** Principal PM

---

## 1. Technical Explanation

The major LLM families each have distinct architectures and sweet spots. **Claude Opus 4.6** and **Gemini 3.1 Pro** currently top intelligence benchmarks, while **GPT-5.2** remains strong for general reasoning. **Llama 4 Scout** (open-weight, Meta) offers a massive 10M token context window and can be self-hosted.

Key tradeoffs to understand:

- **Intelligence vs. Speed:** Opus-class models score highest on reasoning but generate ~50-80 tokens/sec. Smaller models like Granite 3.3 8B hit 530 t/s — 7-10x faster.
- **Cost:** Frontier models cost $15-75 per million tokens (input+output). Open models like DeepSeek or Gemma 3n run at $0.03-0.10/M tokens — 100-500x cheaper.
- **Context windows:** Range from 128K (most GPT/Claude models) to 10M (Llama 4 Scout). Bigger isn't always better — accuracy degrades in the "lost in the middle" zone.
- **Open vs. Closed:** Open-weight models (Llama, Gemma, Qwen) let you fine-tune and self-host. Closed APIs (GPT, Claude, Gemini) offer higher peak quality but vendor lock-in.

## 2. Simple Explanation

Think of LLM models like hiring consultants. **Claude and GPT** are like top-tier McKinsey partners — brilliant, expensive, and you wait for their calendar. **Llama and Gemma** are like talented freelancers — cheaper, flexible, and you can train them on your specific domain. **Gemini** is the in-house Google expert with access to Search and YouTube data.

No single model wins everything. The PM's job is matching the right model to the right task: use a cheap, fast model for autocomplete suggestions, and reserve the expensive frontier model for complex reasoning tasks like financial analysis.

## 3. Real Example

At **Walmart**, imagine building an AI shopping assistant. A PM might architect a **model routing system**: simple queries like "store hours" go to a fast, cheap model (Llama 8B, self-hosted, ~$0.05/M tokens). Complex queries like "plan a week of meals for a diabetic family of four under $80" route to Claude Sonnet or GPT-5 (~$10/M tokens). This single decision — routing by complexity — can cut API costs by 60-80% while maintaining quality where it matters. GitHub Copilot uses a similar approach: a small, fast model handles tab-completion, while a larger model powers chat explanations.

## 4. Key Takeaways

- **No single model wins on all axes** — intelligence, speed, cost, and context size all trade off against each other
- **Model routing is the PM superpower** — match model tier to task complexity to optimize cost and quality simultaneously
- **Open-weight models close the gap fast** — evaluate Llama/Gemma before defaulting to expensive APIs
- **Benchmark on YOUR data, not leaderboards** — generic benchmarks don't predict performance on your specific product use case
- **Plan for model switching** — abstract your LLM layer so you can swap models as the landscape shifts quarterly
