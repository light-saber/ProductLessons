# Daily PM Lesson â€” February 27, 2026

**Category:** AI for PMs

---

## ğŸª Hook

You've shipped an AI feature. You can demo ChatGPT. You've "vibe coded" a prototype. So why does it still feel like you're faking your understanding of AI in product meetings? Because there's a difference between *using* AI and having **AI product sense** â€” and the gap is costing you influence.

---

## ğŸ’¡ Concept: Building AI Product Sense (Not Just AI Literacy)

Tal Raviv and Aman Khan (via Lenny's Newsletter, Feb 2026) make a sharp distinction: most PMs have AI *literacy* â€” they know the buzzwords, they've played with tools. But few have AI *product sense* â€” the intuition for what AI can reliably do, where it breaks, and what's worth building.

Their core insight: **stop using consumer-grade AI UIs to learn AI.** ChatGPT and Copilot hide the messy reality â€” the hallucinations, the prompt fragility, the context limits. You're seeing the demo reel, not the engineering.

Instead, they recommend getting your hands into the plumbing:
- **Use Cursor (or similar) for non-code work.** Write PRDs, analyze data, draft strategies â€” but in an environment where you see the prompts, the failures, and the iterations. You'll quickly learn what AI is actually good at vs. what it fakes.
- **Run the same task across 3 models.** You'll discover that "AI can do X" is wildly model-dependent. This kills the binary thinking ("AI can/can't do this") that leads to bad product decisions.
- **Build a weekly ritual.** Spend 30 minutes each week deliberately stress-testing an AI capability relevant to your product area. Not playing â€” *probing*.

The payoff: when someone in a review says "can't we just use AI for this?", you'll have a calibrated answer instead of a vague "maybe."

---

## ğŸ› ï¸ Apply It

**Today at Walmart:** Pick one repetitive task from your current sprint â€” could be writing acceptance criteria, analyzing customer feedback themes, or drafting a stakeholder update. Do it with AI (Claude, Cursor, whatever), but deliberately push it until it breaks. Note exactly *where* it fails. That failure boundary is your AI product sense growing.

Bonus: share the finding with your team. "I tested AI on X â€” it nailed Y but completely failed at Z." That's the kind of calibrated take that separates a Principal PM from someone who just reads AI newsletters.

---

## ğŸ§© Mini Challenge

**Quick self-assessment:** For the top 3 AI features on your roadmap (or your team's), can you answer these without hedging?

1. What's the failure mode users will hit first?
2. What's the minimum accuracy threshold where the feature is *worse* than no feature?
3. How will you measure if users actually trust the output?

If you can't answer confidently â†’ that's your next deep-dive.

---

**Source:** Tal Raviv & Aman Khan, ["How to Build AI Product Sense"](https://www.lennysnewsletter.com/p/how-to-build-ai-product-sense), Lenny's Newsletter, Feb 2026.
